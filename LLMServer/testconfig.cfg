{
    "host": "0.0.0.0",
    "port": 8080,
    "models": [
        {
            "model": "C:\\Users\\RARI\\.cache\\lm-studio\\models\\TheBloke\\LLaMA-Pro-8B-Instruct-GGUF\\llama-pro-8b-instruct.Q8_0.gguf",
            "model_alias": "llama-pro",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 8,
            "n_batch": 512,
            "n_ctx": 4096
        },
        {
            "model": "C:\\Users\\RARI\\.cache\\lm-studio\\models\\TheBloke\\Mistral-7B-Instruct-v0.1-GGUF\\mistral-7b-instruct-v0.1.Q5_K_M.gguf",
            "model_alias": "mistal",
            "chat_format": "chatml",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 8,
            "n_batch": 512,
            "n_ctx": 16384
        },
        {
            "model": "C:\\Users\\RARI\\.cache\\lm-studio\\models\\TheBloke\\OpenHermes-2.5-Mistral-7B-16k-GGUF\\openhermes-2.5-mistral-7b-16k.Q5_K_M.gguf",
            "model_alias": "mistal16k",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 8,
            "n_batch": 512,
            "n_ctx": 4096
        },
        {
            "model": "C:\\Users\\RARI\\.cache\\lm-studio\\models\\TheBloke\\phi-2-GGUF\\phi-2.Q8_0.gguf",
            "model_alias": "lunatic",
            "n_gpu_layers": -1,
            "offload_kqv": true,
            "n_threads": 8,
            "n_batch": 1024,
            "n_ctx": 2048
        }
    ]
}
